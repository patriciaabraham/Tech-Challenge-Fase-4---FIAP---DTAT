{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TECH CHALLENGE - FASE 4\n",
        "\n",
        "Patrícia Vieira Abraham | Fillipe Júlio de Oliveira Nascimento"
      ],
      "metadata": {
        "id": "Aro1faiaPz1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de Regressão Logística para Previsão do Ibovespa"
      ],
      "metadata": {
        "id": "ebUXKDe1P7lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook tem como objetivo desenvolver e avaliar um modelo de regressão logística para prever a direção do fechamento do Ibovespa no dia seguinte. Especificamente, o modelo tentará prever se o preço de fechamento do Ibovespa no dia t+1 será maior que o preço de fechamento no dia t."
      ],
      "metadata": {
        "id": "kV-Zg0UPQA1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visão Geral do Pipeline\n",
        "O pipeline de machine learning implementado neste notebook segue as seguintes etapas:\n",
        "1.  Carregamento e Pré-processamento dos Dados: Leitura do arquivo CSV do Ibovespa, parse de datas, ordenação temporal e limpeza inicial dos dados.\n",
        "2.  Engenharia de Atributos: Criação de diversas features técnicas e temporais a partir dos dados brutos, como retornos, médias móveis, volatilidades e indicadores de bandas.\n",
        "3.  Definição da Variável Target: Criação da variável binária que indica se o fechamento do dia seguinte foi maior que o fechamento atual.\n",
        "4.  Divisão Temporal dos Dados: Separação dos dados em conjuntos de treino e teste, respeitando a ordem cronológica para evitar vazamento de dados.\n",
        "5.  Construção e Treinamento do Pipeline: Definição de um sklearn.pipeline.Pipeline que inclui um StandardScaler para normalização dos atributos e um modelo de LogisticRegression.\n",
        "6.  Validação Cruzada Temporal: Avaliação do desempenho do pipeline usando TimeSeriesSplit no conjunto de treino.\n",
        "7.  Treinamento Final: Treinamento do pipeline completo no conjunto de treino.\n",
        "8.  Avaliação do Modelo: Geração de predições e avaliação das métricas de desempenho (acurácia, precisão, recall, F1-score, ROC AUC, matriz de confusão, relatório de classificação) no conjunto de teste.\n",
        "9.  Exportação de Artefatos: Salvamento do pipeline treinado, das métricas de avaliação e das predições do conjunto de teste em formatos pickle, json e csv, respectivamente."
      ],
      "metadata": {
        "id": "jeKsMhVlQJoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instruções de Execução\n",
        "\n",
        "Para executar este notebook, siga os passos abaixo:\n",
        "    \n",
        "1.  Pré-requisitos: Certifique-se de ter as bibliotecas Python necessárias instaladas (pandas, numpy, scikit-learn, joblib).\n",
        "Você pode instalá-las via pip:\n",
        "    \n",
        "    !pip install pandas numpy scikit-learn joblib\n",
        "    \n",
        "2.  Estrutura de Pastas: Crie uma pasta chamada data na mesma raiz onde este notebook está salvo. Dentro da pasta data, coloque o arquivo Ibovespa.csv.\n",
        "3.  Execução: Execute todas as células do notebook em ordem. Ao final da execução, uma pasta artifacts será criada (se não existir) contendo:\n",
        "    -   logreg_pipeline.pkl: O pipeline de machine learning treinado.\n",
        "    -   metrics.json: As métricas de avaliação do modelo no conjunto de teste.\n",
        "    -   test_predictions.csv: As predições do modelo para o conjunto de teste, incluindo a variável real, a predição e a probabilidade.\n",
        "    -   pipeline_execution.log: Um arquivo de log com informações sobre a execução do pipeline."
      ],
      "metadata": {
        "id": "pTe566MKQQOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "\n",
        "# --- Configurações Globais ---\n",
        "DATA_PATH = \"data/Dados Históricos - Ibovespa (5).csv\"\n",
        "ARTIFACTS_DIR = \"artifacts\"\n",
        "LOG_FILE = os.path.join(ARTIFACTS_DIR, \"pipeline_execution.log\")\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 30\n",
        "\n",
        "# --- Configuração de Logging ---\n",
        "def setup_logging(log_file: str):\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "\n",
        "    # Get the logger for this module\n",
        "    module_logger = logging.getLogger(__name__)\n",
        "    module_logger.setLevel(logging.INFO) # Set the minimum level for this logger\n",
        "\n",
        "    # Clear existing handlers to prevent duplicate output if run multiple times\n",
        "    if module_logger.handlers:\n",
        "        for handler in list(module_logger.handlers): # Iterate over a copy to safely remove\n",
        "            module_logger.removeHandler(handler)\n",
        "\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # File handler\n",
        "    file_handler = logging.FileHandler(log_file, mode='w')\n",
        "    file_handler.setLevel(logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    module_logger.addHandler(file_handler)\n",
        "\n",
        "    # Stream handler (console)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setLevel(logging.INFO)\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    module_logger.addHandler(stream_handler)\n",
        "\n",
        "# Call the setup function\n",
        "setup_logging(LOG_FILE)\n",
        "# Get the logger instance for use in the rest of the notebook\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Funções Utilitárias ---\n",
        "def _check_file_exists(file_path: str) -> None: #Verifica se um arquivo existe e levanta um erro se não.\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f'Erro: O arquivo {file_path} não foi encontrado. Certifique-se de que ele está na pasta \"data\".')\n",
        "        raise FileNotFoundError(f'Arquivo não encontrado: {file_path}')\n",
        "\n",
        "def _create_directory(path: str) -> None: #Cria um diretório se ele não existir.\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    logger.info(f' Diretório {path} garantido.')\n",
        "\n",
        "def _calculate_bollinger_bands(df: pd.DataFrame, column: str, window: int = 20, num_std: int = 2) -> pd.DataFrame: #Calcula as Bandas de Bollinger.\n",
        "    ma = df[column].rolling(window=window).mean()\n",
        "    std = df[column].rolling(window=window).std()\n",
        "    df[f'{column}_BB_Upper'] = ma + (std * num_std)\n",
        "    df[f'{column}_BB_Lower'] = ma - (std * num_std)\n",
        "    return df\n",
        "\n",
        "def _calculate_atr(df: pd.DataFrame, window: int = 14) -> pd.DataFrame: #Calcula o Average True Range (ATR).\n",
        "    high_low = df['Máxima'] - df['Mínima']\n",
        "    high_prev_close = np.abs(df['Máxima'] - df['Último'].shift(1))\n",
        "    low_prev_close = np.abs(df['Mínima'] - df['Último'].shift(1))\n",
        "    tr = pd.DataFrame({'hl': high_low, 'hpc': high_prev_close, 'lpc': low_prev_close}).max(axis=1)\n",
        "    df['ATR'] = tr.rolling(window=window).mean()\n",
        "    return df\n",
        "\n",
        "def _calculate_parkinson_volatility(df: pd.DataFrame, window: int = 20) -> pd.DataFrame: #Calcula a Volatilidade de Parkinson.\n",
        "    # Parkinson Volatility = sqrt( (1 / (4 * ln(2))) * (ln(High/Low))^2 )\n",
        "    # Usamos uma aproximação ou a forma mais comum de cálculo para dados diários\n",
        "    # Aqui, vamos usar a diferença logarítmica entre máxima e mínima\n",
        "    df['Parkinson_Vol'] = np.sqrt(0.5 * (np.log(df['Máxima'] / df['Mínima'])**2).rolling(window=window).mean())\n",
        "    return df\n",
        "\n",
        "def _engineer_features(df: pd.DataFrame) -> pd.DataFrame: #Cria atributos técnicos e temporais a partir dos dados brutos.\n",
        "    logger.info('Iniciando engenharia de atributos...')\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Retornos e Diferenças\n",
        "    df_copy['pct_change_close'] = df_copy['Último'].pct_change()\n",
        "    df_copy['diff_close'] = df_copy['Último'].diff()\n",
        "    df_copy['high_low_diff'] = df_copy['Máxima'] - df_copy['Mínima']\n",
        "    df_copy['open_close_diff'] = df_copy['Último'] - df_copy['Abertura']\n",
        "\n",
        "    # Médias Móveis Simples (SMA)\n",
        "    for window in [3, 7, 14, 21, 30]:\n",
        "        df_copy[f'SMA_{window}'] = df_copy['Último'].rolling(window=window).mean()\n",
        "        df_copy[f'SMA_Vol_{window}'] = df_copy['Var%'].rolling(window=window).mean()\n",
        "\n",
        "    # Volatilidade (desvio padrão)\n",
        "    for window in [7, 14, 21]:\n",
        "        df_copy[f'Vol_Std_{window}'] = df_copy['Último'].rolling(window=window).std()\n",
        "\n",
        "    # Indicadores Avançados\n",
        "    df_copy = _calculate_parkinson_volatility(df_copy)\n",
        "    df_copy = _calculate_atr(df_copy)\n",
        "    df_copy = _calculate_bollinger_bands(df_copy, 'Último')\n",
        "\n",
        "    # Atributos Temporais\n",
        "    df_copy['day_of_week'] = df_copy['Data'].dt.dayofweek\n",
        "    df_copy['day_of_month'] = df_copy['Data'].dt.day\n",
        "    df_copy['month'] = df_copy['Data'].dt.month\n",
        "    df_copy['year'] = df_copy['Data'].dt.year\n",
        "\n",
        "    logger.info('Engenharia de atributos concluída.')\n",
        "    return df_copy\n",
        "\n",
        "def _evaluate_model(y_true: pd.Series, y_pred: np.ndarray, y_proba: np.ndarray) -> dict: #Calcula e retorna as principais métricas de avaliação do modelo.\n",
        "    logger.info('Calculando métricas de avaliação...')\n",
        "    metrics = {\n",
        "        'accuracy_test': accuracy_score(y_true, y_pred),\n",
        "        'precision_test': precision_score(y_true, y_pred),\n",
        "        'recall_test': recall_score(y_true, y_pred),\n",
        "        'f1_test': f1_score(y_true, y_pred),\n",
        "        'roc_auc_test': roc_auc_score(y_true, y_proba),\n",
        "        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(), # Conversão para lista no JSON\n",
        "        'classification_report': classification_report(y_true, y_pred)\n",
        "    }\n",
        "    logger.info('Métricas calculadas com sucesso.')\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "VHZ6kGvuTyzj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Carregamento e Pré-processamento dos Dados"
      ],
      "metadata": {
        "id": "lCKgZuvob3z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando carregamento e pré-processamento dos dados.')\n",
        "_check_file_exists(DATA_PATH)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    logger.info(f'Arquivo {DATA_PATH} carregado com sucesso. Shape inicial: {df.shape}')\n",
        "\n",
        "    # Parse de datas e ordenação\n",
        "    df['Data'] = pd.to_datetime(df['Data'], format=\"%d.%m.%Y\")\n",
        "    df = df.sort_values('Data').reset_index(drop=True)\n",
        "    logger.info('Coluna \"Data\" convertida e DataFrame ordenado temporalmente.')\n",
        "\n",
        "    # Limpeza da coluna 'Var%'\n",
        "    if 'Var%' in df.columns:\n",
        "        df['Var%'] = df['Var%'].str.replace(',', '.', regex=False).str.replace('%', '', regex=False).astype(float) / 100\n",
        "        logger.info('Coluna \"Var%\" limpa e convertida para float.')\n",
        "    else:\n",
        "        logger.warning('Coluna \"Var%\" não encontrada no DataFrame.')\n",
        "\n",
        "    # Remoção da coluna 'Vol.'\n",
        "    if 'Vol.' in df.columns:\n",
        "        df = df.drop(columns=['Vol.'])\n",
        "        logger.info('Coluna \"Vol.\" removida.')\n",
        "    else:\n",
        "        logger.warning('Coluna \"Vol.\" não encontrada no DataFrame.')\n",
        "\n",
        "    logger.info(f'Pré-processamento inicial concluído. Shape atual: {df.shape}')\n",
        "    logger.info('Primeiras 5 linhas do DataFrame após pré-processamento inicial:' + df.head().to_string())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante o carregamento ou pré-processamento inicial dos dados: {e}')\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bexIjio1cMiU",
        "outputId": "682d9b6f-bc30-486d-e31b-41ee7ca3c1c0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:03:45,387 - INFO - Iniciando carregamento e pré-processamento dos dados.\n",
            "INFO:__main__:Iniciando carregamento e pré-processamento dos dados.\n",
            "2026-01-16 03:03:45,402 - INFO - Arquivo data/Dados Históricos - Ibovespa (5).csv carregado com sucesso. Shape inicial: (4974, 7)\n",
            "INFO:__main__:Arquivo data/Dados Históricos - Ibovespa (5).csv carregado com sucesso. Shape inicial: (4974, 7)\n",
            "2026-01-16 03:03:45,421 - INFO - Coluna \"Data\" convertida e DataFrame ordenado temporalmente.\n",
            "INFO:__main__:Coluna \"Data\" convertida e DataFrame ordenado temporalmente.\n",
            "2026-01-16 03:03:45,429 - INFO - Coluna \"Var%\" limpa e convertida para float.\n",
            "INFO:__main__:Coluna \"Var%\" limpa e convertida para float.\n",
            "2026-01-16 03:03:45,432 - INFO - Coluna \"Vol.\" removida.\n",
            "INFO:__main__:Coluna \"Vol.\" removida.\n",
            "2026-01-16 03:03:45,434 - INFO - Pré-processamento inicial concluído. Shape atual: (4974, 6)\n",
            "INFO:__main__:Pré-processamento inicial concluído. Shape atual: (4974, 6)\n",
            "2026-01-16 03:03:45,441 - INFO - Primeiras 5 linhas do DataFrame após pré-processamento inicial:        Data  Último  Abertura  Máxima  Mínima    Var%\n",
            "0 2005-06-01  25.949    25.209  25.992  25.209  0.0294\n",
            "1 2005-06-02  26.640    25.950  26.755  25.930  0.0266\n",
            "2 2005-06-03  26.366    26.642  26.692  26.145 -0.0103\n",
            "3 2005-06-06  25.556    26.363  26.363  25.217 -0.0307\n",
            "4 2005-06-07  25.026    25.556  25.556  24.933 -0.0207\n",
            "INFO:__main__:Primeiras 5 linhas do DataFrame após pré-processamento inicial:        Data  Último  Abertura  Máxima  Mínima    Var%\n",
            "0 2005-06-01  25.949    25.209  25.992  25.209  0.0294\n",
            "1 2005-06-02  26.640    25.950  26.755  25.930  0.0266\n",
            "2 2005-06-03  26.366    26.642  26.692  26.145 -0.0103\n",
            "3 2005-06-06  25.556    26.363  26.363  25.217 -0.0307\n",
            "4 2005-06-07  25.026    25.556  25.556  24.933 -0.0207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Engenharia de Atributos e Definição da Variável Target"
      ],
      "metadata": {
        "id": "523I29XBe68c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando engenharia de atributos e definição da variável target.')\n",
        "\n",
        "try:\n",
        "    # Criar atributos\n",
        "    df_processed = _engineer_features(df.copy())\n",
        "\n",
        "    # Criar variável target: 1 se o fechamento de amanhã for maior que o de hoje, 0 caso contrário\n",
        "    df_processed['target'] = (df_processed['Último'].shift(-1) > df_processed['Último']).astype(int)\n",
        "    logger.info('Variável \"target\" criada (1 se fechamento de amanhã > hoje, 0 caso contrário).')\n",
        "\n",
        "    # Remover NaNs gerados pela engenharia de atributos e pelo shift da target\n",
        "    initial_rows = df_processed.shape[0]\n",
        "    df_processed.dropna(inplace=True)\n",
        "    rows_dropped = initial_rows - df_processed.shape[0]\n",
        "    logger.info(f'{rows_dropped} linhas com valores NaN foram removidas após engenharia de atributos e criação da target.')\n",
        "\n",
        "    logger.info(f'Engenharia de atributos e target concluída. Shape final: {df_processed.shape}')\n",
        "    logger.info('Colunas do DataFrame após engenharia de atributos:' + str(df_processed.columns.tolist()))\n",
        "    logger.info('Primeiras 5 linhas do DataFrame após engenharia de atributos e remoção de NaNs:' + df_processed.head().to_string())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante a engenharia de atributos ou criação da target: {e}')\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtWAYz76fCvr",
        "outputId": "7c142c0c-af13-4c66-d0f0-f4926bc8756f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:04:35,858 - INFO - Iniciando engenharia de atributos e definição da variável target.\n",
            "INFO:__main__:Iniciando engenharia de atributos e definição da variável target.\n",
            "2026-01-16 03:04:35,864 - INFO - Iniciando engenharia de atributos...\n",
            "INFO:__main__:Iniciando engenharia de atributos...\n",
            "2026-01-16 03:04:35,887 - INFO - Engenharia de atributos concluída.\n",
            "INFO:__main__:Engenharia de atributos concluída.\n",
            "2026-01-16 03:04:35,890 - INFO - Variável \"target\" criada (1 se fechamento de amanhã > hoje, 0 caso contrário).\n",
            "INFO:__main__:Variável \"target\" criada (1 se fechamento de amanhã > hoje, 0 caso contrário).\n",
            "2026-01-16 03:04:35,897 - INFO - 29 linhas com valores NaN foram removidas após engenharia de atributos e criação da target.\n",
            "INFO:__main__:29 linhas com valores NaN foram removidas após engenharia de atributos e criação da target.\n",
            "2026-01-16 03:04:35,899 - INFO - Engenharia de atributos e target concluída. Shape final: (4945, 32)\n",
            "INFO:__main__:Engenharia de atributos e target concluída. Shape final: (4945, 32)\n",
            "2026-01-16 03:04:35,901 - INFO - Colunas do DataFrame após engenharia de atributos:['Data', 'Último', 'Abertura', 'Máxima', 'Mínima', 'Var%', 'pct_change_close', 'diff_close', 'high_low_diff', 'open_close_diff', 'SMA_3', 'SMA_Vol_3', 'SMA_7', 'SMA_Vol_7', 'SMA_14', 'SMA_Vol_14', 'SMA_21', 'SMA_Vol_21', 'SMA_30', 'SMA_Vol_30', 'Vol_Std_7', 'Vol_Std_14', 'Vol_Std_21', 'Parkinson_Vol', 'ATR', 'Último_BB_Upper', 'Último_BB_Lower', 'day_of_week', 'day_of_month', 'month', 'year', 'target']\n",
            "INFO:__main__:Colunas do DataFrame após engenharia de atributos:['Data', 'Último', 'Abertura', 'Máxima', 'Mínima', 'Var%', 'pct_change_close', 'diff_close', 'high_low_diff', 'open_close_diff', 'SMA_3', 'SMA_Vol_3', 'SMA_7', 'SMA_Vol_7', 'SMA_14', 'SMA_Vol_14', 'SMA_21', 'SMA_Vol_21', 'SMA_30', 'SMA_Vol_30', 'Vol_Std_7', 'Vol_Std_14', 'Vol_Std_21', 'Parkinson_Vol', 'ATR', 'Último_BB_Upper', 'Último_BB_Lower', 'day_of_week', 'day_of_month', 'month', 'year', 'target']\n",
            "2026-01-16 03:04:35,916 - INFO - Primeiras 5 linhas do DataFrame após engenharia de atributos e remoção de NaNs:         Data  Último  Abertura  Máxima  Mínima    Var%  pct_change_close  diff_close  high_low_diff  open_close_diff      SMA_3  SMA_Vol_3      SMA_7  SMA_Vol_7     SMA_14  SMA_Vol_14     SMA_21  SMA_Vol_21     SMA_30  SMA_Vol_30  Vol_Std_7  Vol_Std_14  Vol_Std_21  Parkinson_Vol       ATR  Último_BB_Upper  Último_BB_Lower  day_of_week  day_of_month  month  year  target\n",
            "29 2005-07-12  25.536    25.027  25.576  24.932  0.0208          0.020787       0.520          0.644            0.509  24.991667   0.014667  24.808857   0.001371  24.955000   -0.000286  25.232619    0.001314  25.282033    0.000577   0.409913    0.340894    0.501066       0.014405  0.471286        26.206709        24.207391            1            12      7  2005       1\n",
            "30 2005-07-13  25.856    25.557  26.043  25.557  0.0125          0.012531       0.320          0.486            0.299  25.469333   0.019200  24.924714   0.004657  25.029286    0.003007  25.237952    0.000300  25.278933    0.000013   0.570812    0.413794    0.507338       0.013772  0.444071        26.260546        24.191054            2            13      7  2005       1\n",
            "31 2005-07-14  25.920    25.859  26.142  25.725  0.0025          0.002475       0.064          0.417            0.061  25.770667   0.011933  25.102571   0.007129  25.100929    0.002893  25.258857    0.000905  25.254933   -0.000790   0.666053    0.475139    0.526535       0.013768  0.453429        26.289609        24.178891            3            14      7  2005       0\n",
            "32 2005-07-15  25.222    25.916  25.916  25.222 -0.0269         -0.026929      -0.698          0.694           -0.694  25.666000  -0.003967  25.203286   0.004200  25.100643    0.000086  25.233667   -0.000881  25.216800   -0.001343   0.614020    0.475059    0.514325       0.014078  0.455714        26.165675        24.215725            4            15      7  2005       1\n",
            "33 2005-07-18  25.321    25.222  25.412  24.916  0.0039          0.003925       0.099          0.496            0.099  25.487667  -0.006833  25.327714   0.005143  25.104929    0.000264  25.196905   -0.001329  25.208967   -0.000190   0.516424    0.476884    0.475994       0.014234  0.465714        26.045858        24.263042            0            18      7  2005       0\n",
            "INFO:__main__:Primeiras 5 linhas do DataFrame após engenharia de atributos e remoção de NaNs:         Data  Último  Abertura  Máxima  Mínima    Var%  pct_change_close  diff_close  high_low_diff  open_close_diff      SMA_3  SMA_Vol_3      SMA_7  SMA_Vol_7     SMA_14  SMA_Vol_14     SMA_21  SMA_Vol_21     SMA_30  SMA_Vol_30  Vol_Std_7  Vol_Std_14  Vol_Std_21  Parkinson_Vol       ATR  Último_BB_Upper  Último_BB_Lower  day_of_week  day_of_month  month  year  target\n",
            "29 2005-07-12  25.536    25.027  25.576  24.932  0.0208          0.020787       0.520          0.644            0.509  24.991667   0.014667  24.808857   0.001371  24.955000   -0.000286  25.232619    0.001314  25.282033    0.000577   0.409913    0.340894    0.501066       0.014405  0.471286        26.206709        24.207391            1            12      7  2005       1\n",
            "30 2005-07-13  25.856    25.557  26.043  25.557  0.0125          0.012531       0.320          0.486            0.299  25.469333   0.019200  24.924714   0.004657  25.029286    0.003007  25.237952    0.000300  25.278933    0.000013   0.570812    0.413794    0.507338       0.013772  0.444071        26.260546        24.191054            2            13      7  2005       1\n",
            "31 2005-07-14  25.920    25.859  26.142  25.725  0.0025          0.002475       0.064          0.417            0.061  25.770667   0.011933  25.102571   0.007129  25.100929    0.002893  25.258857    0.000905  25.254933   -0.000790   0.666053    0.475139    0.526535       0.013768  0.453429        26.289609        24.178891            3            14      7  2005       0\n",
            "32 2005-07-15  25.222    25.916  25.916  25.222 -0.0269         -0.026929      -0.698          0.694           -0.694  25.666000  -0.003967  25.203286   0.004200  25.100643    0.000086  25.233667   -0.000881  25.216800   -0.001343   0.614020    0.475059    0.514325       0.014078  0.455714        26.165675        24.215725            4            15      7  2005       1\n",
            "33 2005-07-18  25.321    25.222  25.412  24.916  0.0039          0.003925       0.099          0.496            0.099  25.487667  -0.006833  25.327714   0.005143  25.104929    0.000264  25.196905   -0.001329  25.208967   -0.000190   0.516424    0.476884    0.475994       0.014234  0.465714        26.045858        24.263042            0            18      7  2005       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Divisão Temporal dos Dados"
      ],
      "metadata": {
        "id": "oz04Ckf1gTmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando divisão temporal dos dados em treino e teste.')\n",
        "\n",
        "try:\n",
        "    # Definir features (X) e target (y)\n",
        "    # Excluir colunas que não são features ou são a própria target\n",
        "    features_to_exclude = ['Data', 'Fechamento', 'Abertura', 'Máxima', 'Mínima', 'target']\n",
        "    X = df_processed.drop(columns=[col for col in features_to_exclude if col in df_processed.columns])\n",
        "    y = df_processed['target']\n",
        "\n",
        "    # Armazenar a coluna 'Data' para o conjunto de teste para exportação futura\n",
        "    data_test_series = df_processed['Data'].iloc[- TEST_SIZE:]\n",
        "\n",
        "    # Divisão temporal (sem shuffle)\n",
        "    train_size = int(len(df_processed) - TEST_SIZE)\n",
        "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "    logger.info(f'Divisão de dados concluída. Base de teste: {TEST_SIZE}')\n",
        "    logger.info(f'Shape de X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
        "    logger.info(f'Shape de X_test: {X_test.shape}, y_test: {y_test.shape}')\n",
        "    logger.info('Primeiras 5 linhas de X_train:' + X_train.head().to_string())\n",
        "    logger.info('Primeiras 5 linhas de y_train:' + y_train.head().to_string())\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante a divisão temporal dos dados: {e}')\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayKjBhlZgbU8",
        "outputId": "9fd97628-b906-4a40-e7bf-9a69b00d6118"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:04:57,219 - INFO - Iniciando divisão temporal dos dados em treino e teste.\n",
            "INFO:__main__:Iniciando divisão temporal dos dados em treino e teste.\n",
            "2026-01-16 03:04:57,524 - INFO - Divisão de dados concluída. Base de teste: 30\n",
            "INFO:__main__:Divisão de dados concluída. Base de teste: 30\n",
            "2026-01-16 03:04:57,526 - INFO - Shape de X_train: (4915, 27), y_train: (4915,)\n",
            "INFO:__main__:Shape de X_train: (4915, 27), y_train: (4915,)\n",
            "2026-01-16 03:04:57,530 - INFO - Shape de X_test: (30, 27), y_test: (30,)\n",
            "INFO:__main__:Shape de X_test: (30, 27), y_test: (30,)\n",
            "2026-01-16 03:04:57,544 - INFO - Primeiras 5 linhas de X_train:    Último    Var%  pct_change_close  diff_close  high_low_diff  open_close_diff      SMA_3  SMA_Vol_3      SMA_7  SMA_Vol_7     SMA_14  SMA_Vol_14     SMA_21  SMA_Vol_21     SMA_30  SMA_Vol_30  Vol_Std_7  Vol_Std_14  Vol_Std_21  Parkinson_Vol       ATR  Último_BB_Upper  Último_BB_Lower  day_of_week  day_of_month  month  year\n",
            "29  25.536  0.0208          0.020787       0.520          0.644            0.509  24.991667   0.014667  24.808857   0.001371  24.955000   -0.000286  25.232619    0.001314  25.282033    0.000577   0.409913    0.340894    0.501066       0.014405  0.471286        26.206709        24.207391            1            12      7  2005\n",
            "30  25.856  0.0125          0.012531       0.320          0.486            0.299  25.469333   0.019200  24.924714   0.004657  25.029286    0.003007  25.237952    0.000300  25.278933    0.000013   0.570812    0.413794    0.507338       0.013772  0.444071        26.260546        24.191054            2            13      7  2005\n",
            "31  25.920  0.0025          0.002475       0.064          0.417            0.061  25.770667   0.011933  25.102571   0.007129  25.100929    0.002893  25.258857    0.000905  25.254933   -0.000790   0.666053    0.475139    0.526535       0.013768  0.453429        26.289609        24.178891            3            14      7  2005\n",
            "32  25.222 -0.0269         -0.026929      -0.698          0.694           -0.694  25.666000  -0.003967  25.203286   0.004200  25.100643    0.000086  25.233667   -0.000881  25.216800   -0.001343   0.614020    0.475059    0.514325       0.014078  0.455714        26.165675        24.215725            4            15      7  2005\n",
            "33  25.321  0.0039          0.003925       0.099          0.496            0.099  25.487667  -0.006833  25.327714   0.005143  25.104929    0.000264  25.196905   -0.001329  25.208967   -0.000190   0.516424    0.476884    0.475994       0.014234  0.465714        26.045858        24.263042            0            18      7  2005\n",
            "INFO:__main__:Primeiras 5 linhas de X_train:    Último    Var%  pct_change_close  diff_close  high_low_diff  open_close_diff      SMA_3  SMA_Vol_3      SMA_7  SMA_Vol_7     SMA_14  SMA_Vol_14     SMA_21  SMA_Vol_21     SMA_30  SMA_Vol_30  Vol_Std_7  Vol_Std_14  Vol_Std_21  Parkinson_Vol       ATR  Último_BB_Upper  Último_BB_Lower  day_of_week  day_of_month  month  year\n",
            "29  25.536  0.0208          0.020787       0.520          0.644            0.509  24.991667   0.014667  24.808857   0.001371  24.955000   -0.000286  25.232619    0.001314  25.282033    0.000577   0.409913    0.340894    0.501066       0.014405  0.471286        26.206709        24.207391            1            12      7  2005\n",
            "30  25.856  0.0125          0.012531       0.320          0.486            0.299  25.469333   0.019200  24.924714   0.004657  25.029286    0.003007  25.237952    0.000300  25.278933    0.000013   0.570812    0.413794    0.507338       0.013772  0.444071        26.260546        24.191054            2            13      7  2005\n",
            "31  25.920  0.0025          0.002475       0.064          0.417            0.061  25.770667   0.011933  25.102571   0.007129  25.100929    0.002893  25.258857    0.000905  25.254933   -0.000790   0.666053    0.475139    0.526535       0.013768  0.453429        26.289609        24.178891            3            14      7  2005\n",
            "32  25.222 -0.0269         -0.026929      -0.698          0.694           -0.694  25.666000  -0.003967  25.203286   0.004200  25.100643    0.000086  25.233667   -0.000881  25.216800   -0.001343   0.614020    0.475059    0.514325       0.014078  0.455714        26.165675        24.215725            4            15      7  2005\n",
            "33  25.321  0.0039          0.003925       0.099          0.496            0.099  25.487667  -0.006833  25.327714   0.005143  25.104929    0.000264  25.196905   -0.001329  25.208967   -0.000190   0.516424    0.476884    0.475994       0.014234  0.465714        26.045858        24.263042            0            18      7  2005\n",
            "2026-01-16 03:04:57,546 - INFO - Primeiras 5 linhas de y_train:29    1\n",
            "30    1\n",
            "31    0\n",
            "32    1\n",
            "33    0\n",
            "INFO:__main__:Primeiras 5 linhas de y_train:29    1\n",
            "30    1\n",
            "31    0\n",
            "32    1\n",
            "33    0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Construção e Treinamento do Pipeline"
      ],
      "metadata": {
        "id": "r5LPHur-hNSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando construção e treinamento do pipeline de machine learning.')\n",
        "\n",
        "try:\n",
        "    # Definir o pipeline: StandardScaler + LogisticRegression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', LogisticRegression(\n",
        "            solver='saga', # 'liblinear' ou 'saga' são bons para datasets menores e L1/L2\n",
        "            max_iter=2000, # Aumentar max_iter para garantir convergência\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1 # Usar todos os cores disponíveis\n",
        "        ))\n",
        "    ])\n",
        "    logger.info('Pipeline definido: StandardScaler -> LogisticRegression.')\n",
        "\n",
        "    # Validação cruzada com TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5) # 5 splits para validação temporal\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
        "    logger.info(f'Scores de validação cruzada (TimeSeriesSplit) no treino: {cv_scores}')\n",
        "    logger.info(f'Acurácia média da validação cruzada: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})')\n",
        "\n",
        "    # Treinar o pipeline no conjunto de treino completo\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    logger.info('Pipeline treinado com sucesso no conjunto de treino completo.')\n",
        "\n",
        "    # Capturar acurácia de treino para métricas\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "    logger.info(f'Acurácia no conjunto de treino: {accuracy_train:.4f}')\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante a construção ou treinamento do pipeline: {e}')\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlpjULf6hPCD",
        "outputId": "cc3a6920-04fa-439e-e9b5-9565476a097d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:05:10,232 - INFO - Iniciando construção e treinamento do pipeline de machine learning.\n",
            "INFO:__main__:Iniciando construção e treinamento do pipeline de machine learning.\n",
            "2026-01-16 03:05:10,236 - INFO - Pipeline definido: StandardScaler -> LogisticRegression.\n",
            "INFO:__main__:Pipeline definido: StandardScaler -> LogisticRegression.\n",
            "2026-01-16 03:05:20,076 - INFO - Scores de validação cruzada (TimeSeriesSplit) no treino: [0.45909646 0.48351648 0.47496947 0.54212454 0.47008547]\n",
            "INFO:__main__:Scores de validação cruzada (TimeSeriesSplit) no treino: [0.45909646 0.48351648 0.47496947 0.54212454 0.47008547]\n",
            "2026-01-16 03:05:20,078 - INFO - Acurácia média da validação cruzada: 0.4860 (+/- 0.0292)\n",
            "INFO:__main__:Acurácia média da validação cruzada: 0.4860 (+/- 0.0292)\n",
            "2026-01-16 03:05:24,665 - INFO - Pipeline treinado com sucesso no conjunto de treino completo.\n",
            "INFO:__main__:Pipeline treinado com sucesso no conjunto de treino completo.\n",
            "2026-01-16 03:05:24,675 - INFO - Acurácia no conjunto de treino: 0.5251\n",
            "INFO:__main__:Acurácia no conjunto de treino: 0.5251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Avaliação do Modelo no Conjunto de Teste"
      ],
      "metadata": {
        "id": "2J-oCNFkiN2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando avaliação do modelo no conjunto de teste.')\n",
        "\n",
        "try:\n",
        "    # Fazer predições no conjunto de teste\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1] # Probabilidade da classe positiva (1)\n",
        "    logger.info('Predições e probabilidades geradas para o conjunto de teste.')\n",
        "\n",
        "    # Calcular métricas de avaliação\n",
        "    test_metrics = _evaluate_model(y_test, y_pred, y_proba)\n",
        "    test_metrics['accuracy_train'] = accuracy_train # Adiciona a acurácia de treino às métricas\n",
        "\n",
        "    logger.info('Métricas de Teste:' +\n",
        "                f'  Acurácia: {test_metrics['accuracy_test']:.4f}' +\n",
        "                f'  Precisão: {test_metrics['precision_test']:.4f}' +\n",
        "                f'  Recall: {test_metrics['recall_test']:.4f}' +\n",
        "                f'  F1-Score: {test_metrics['f1_test']:.4f}' +\n",
        "                f'  ROC AUC: {test_metrics['roc_auc_test']:.4f}')\n",
        "    logger.info('Relatório de Classificação:' + test_metrics['classification_report'])\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante a avaliação do modelo no conjunto de teste: {e}')\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg2dVeBbiUZj",
        "outputId": "b7e3217d-3a3b-4575-cd38-7612f8b342de"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:05:24,686 - INFO - Iniciando avaliação do modelo no conjunto de teste.\n",
            "INFO:__main__:Iniciando avaliação do modelo no conjunto de teste.\n",
            "2026-01-16 03:05:24,697 - INFO - Predições e probabilidades geradas para o conjunto de teste.\n",
            "INFO:__main__:Predições e probabilidades geradas para o conjunto de teste.\n",
            "2026-01-16 03:05:24,699 - INFO - Calculando métricas de avaliação...\n",
            "INFO:__main__:Calculando métricas de avaliação...\n",
            "2026-01-16 03:05:24,720 - INFO - Métricas calculadas com sucesso.\n",
            "INFO:__main__:Métricas calculadas com sucesso.\n",
            "2026-01-16 03:05:24,723 - INFO - Métricas de Teste:  Acurácia: 0.6000  Precisão: 0.5000  Recall: 0.6667  F1-Score: 0.5714  ROC AUC: 0.5602\n",
            "INFO:__main__:Métricas de Teste:  Acurácia: 0.6000  Precisão: 0.5000  Recall: 0.6667  F1-Score: 0.5714  ROC AUC: 0.5602\n",
            "2026-01-16 03:05:24,724 - INFO - Relatório de Classificação:              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.56      0.62        18\n",
            "           1       0.50      0.67      0.57        12\n",
            "\n",
            "    accuracy                           0.60        30\n",
            "   macro avg       0.61      0.61      0.60        30\n",
            "weighted avg       0.63      0.60      0.60        30\n",
            "\n",
            "INFO:__main__:Relatório de Classificação:              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.56      0.62        18\n",
            "           1       0.50      0.67      0.57        12\n",
            "\n",
            "    accuracy                           0.60        30\n",
            "   macro avg       0.61      0.61      0.60        30\n",
            "weighted avg       0.63      0.60      0.60        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Exportação de Artefatos"
      ],
      "metadata": {
        "id": "Gwbk3mV6ibTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('Iniciando exportação de artefatos.')\n",
        "\n",
        "try:\n",
        "    _create_directory(ARTIFACTS_DIR)\n",
        "\n",
        "    # 1. Exportar o pipeline completo em formato pickle\n",
        "    pipeline_path = os.path.join(ARTIFACTS_DIR, \"logreg_pipeline.pkl\")\n",
        "    joblib.dump(pipeline, pipeline_path)\n",
        "    logger.info(f'Pipeline salvo em: {pipeline_path}')\n",
        "\n",
        "    # 2. Exportar as principais métricas em JSON\n",
        "    metrics_path = os.path.join(ARTIFACTS_DIR, 'metrics.json')\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(test_metrics, f, indent=4)\n",
        "    logger.info(f'Métricas salvas em: {metrics_path}')\n",
        "\n",
        "    # 3. Exportar as predições sobre o conjunto de teste em CSV\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Data': data_test_series.values, # Usar a série de datas salva anteriormente\n",
        "        'y_true': y_test.values,\n",
        "        'y_pred': y_pred,\n",
        "        'y_proba': y_proba\n",
        "    })\n",
        "    predictions_path = os.path.join(ARTIFACTS_DIR, \"test_predictions.csv\")\n",
        "    predictions_df.to_csv(predictions_path, index=False)\n",
        "    logger.info(f'Predições do conjunto de teste salvas em: {predictions_path}')\n",
        "\n",
        "    logger.info('Exportação de todos os artefatos concluída com sucesso.')\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f'Erro durante a exportação de artefatos: {e}')\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeuDRnH3iiO7",
        "outputId": "c918cf43-a4d3-446c-8bf8-6b6ea721bdc0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-16 03:05:28,116 - INFO - Iniciando exportação de artefatos.\n",
            "INFO:__main__:Iniciando exportação de artefatos.\n",
            "2026-01-16 03:05:28,120 - INFO -  Diretório artifacts garantido.\n",
            "INFO:__main__: Diretório artifacts garantido.\n",
            "2026-01-16 03:05:28,126 - INFO - Pipeline salvo em: artifacts/logreg_pipeline.pkl\n",
            "INFO:__main__:Pipeline salvo em: artifacts/logreg_pipeline.pkl\n",
            "2026-01-16 03:05:28,130 - INFO - Métricas salvas em: artifacts/metrics.json\n",
            "INFO:__main__:Métricas salvas em: artifacts/metrics.json\n",
            "2026-01-16 03:05:28,136 - INFO - Predições do conjunto de teste salvas em: artifacts/test_predictions.csv\n",
            "INFO:__main__:Predições do conjunto de teste salvas em: artifacts/test_predictions.csv\n",
            "2026-01-16 03:05:28,139 - INFO - Exportação de todos os artefatos concluída com sucesso.\n",
            "INFO:__main__:Exportação de todos os artefatos concluída com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyK7ickXnJn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}